{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                 # linear algebra\n",
    "import pandas as pd                # data processing\n",
    "import matplotlib.pyplot as plt    # basic plotting library\n",
    "import seaborn as sns              # more advanced visual plotting library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>fClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "      <td>19020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>53.250154</td>\n",
       "      <td>22.180966</td>\n",
       "      <td>2.825017</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>0.214657</td>\n",
       "      <td>-4.331745</td>\n",
       "      <td>10.545545</td>\n",
       "      <td>0.249726</td>\n",
       "      <td>27.645707</td>\n",
       "      <td>193.818026</td>\n",
       "      <td>0.648370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.364855</td>\n",
       "      <td>18.346056</td>\n",
       "      <td>0.472599</td>\n",
       "      <td>0.182813</td>\n",
       "      <td>0.110511</td>\n",
       "      <td>59.206062</td>\n",
       "      <td>51.000118</td>\n",
       "      <td>20.827439</td>\n",
       "      <td>26.103621</td>\n",
       "      <td>74.731787</td>\n",
       "      <td>0.477492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.283500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.941300</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-457.916100</td>\n",
       "      <td>-331.780000</td>\n",
       "      <td>-205.894700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.282600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.336000</td>\n",
       "      <td>11.863800</td>\n",
       "      <td>2.477100</td>\n",
       "      <td>0.235800</td>\n",
       "      <td>0.128475</td>\n",
       "      <td>-20.586550</td>\n",
       "      <td>-12.842775</td>\n",
       "      <td>-10.849375</td>\n",
       "      <td>5.547925</td>\n",
       "      <td>142.492250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.147700</td>\n",
       "      <td>17.139900</td>\n",
       "      <td>2.739600</td>\n",
       "      <td>0.354150</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>4.013050</td>\n",
       "      <td>15.314100</td>\n",
       "      <td>0.666200</td>\n",
       "      <td>17.679500</td>\n",
       "      <td>191.851450</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.122175</td>\n",
       "      <td>24.739475</td>\n",
       "      <td>3.101600</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.285225</td>\n",
       "      <td>24.063700</td>\n",
       "      <td>35.837800</td>\n",
       "      <td>10.946425</td>\n",
       "      <td>45.883550</td>\n",
       "      <td>240.563825</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>334.177000</td>\n",
       "      <td>256.382000</td>\n",
       "      <td>5.323300</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>575.240700</td>\n",
       "      <td>238.321000</td>\n",
       "      <td>179.851000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>495.561000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            fLength        fWidth         fSize         fConc        fConc1  \\\n",
       "count  19020.000000  19020.000000  19020.000000  19020.000000  19020.000000   \n",
       "mean      53.250154     22.180966      2.825017      0.380327      0.214657   \n",
       "std       42.364855     18.346056      0.472599      0.182813      0.110511   \n",
       "min        4.283500      0.000000      1.941300      0.013100      0.000300   \n",
       "25%       24.336000     11.863800      2.477100      0.235800      0.128475   \n",
       "50%       37.147700     17.139900      2.739600      0.354150      0.196500   \n",
       "75%       70.122175     24.739475      3.101600      0.503700      0.285225   \n",
       "max      334.177000    256.382000      5.323300      0.893000      0.675200   \n",
       "\n",
       "              fAsym       fM3Long      fM3Trans        fAlpha         fDist  \\\n",
       "count  19020.000000  19020.000000  19020.000000  19020.000000  19020.000000   \n",
       "mean      -4.331745     10.545545      0.249726     27.645707    193.818026   \n",
       "std       59.206062     51.000118     20.827439     26.103621     74.731787   \n",
       "min     -457.916100   -331.780000   -205.894700      0.000000      1.282600   \n",
       "25%      -20.586550    -12.842775    -10.849375      5.547925    142.492250   \n",
       "50%        4.013050     15.314100      0.666200     17.679500    191.851450   \n",
       "75%       24.063700     35.837800     10.946425     45.883550    240.563825   \n",
       "max      575.240700    238.321000    179.851000     90.000000    495.561000   \n",
       "\n",
       "             fClass  \n",
       "count  19020.000000  \n",
       "mean       0.648370  \n",
       "std        0.477492  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\", \"fAsym\", \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"fClass\"]\n",
    "df = pd.read_csv('magic04.data', names=headers)\n",
    "mapping = {'g': 1, 'h': 0}\n",
    "df[\"fClass\"] = df[\"fClass\"].map(mapping)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rescaled = pd.DataFrame()\n",
    "headers = [\"fLength\", \"fWidth\", \"fConc\", \"fConc1\", \"fAsym\", \"fM3Long\", \"fM3Trans\", \"fDist\",\"fAlpha\", \"fSize\",\"fClass\"]\n",
    "rescaled_headers = [ \"rescaled_length\", \"rescaled_width\",\"rescaled_conc\",\"rescaled_conc1\"  ,\"rescaled_asym\",\"rescaled_M3long\" ,\"rescaled_M3trans\",\"rescaled_dist\" ,\"alpha\",\"size\",\"rad\"]\n",
    "for i in range(0,8):\n",
    "    df_rescaled.insert(i, rescaled_headers[i] ,df[headers[i]]/df[\"fSize\"])\n",
    "df_rescaled.insert(8, \"alpha\" ,df[\"fAlpha\"])\n",
    "df_rescaled.insert(9, \"size\" ,df[\"fSize\"])\n",
    "df_rescaled.insert(10, \"rad\" ,df[\"fClass\"])\n",
    "df_rescaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['fAlpha'] == 0]))\n",
    "print(len(df[df['fAlpha'] == 90]))\n",
    "print(len(df[df['fSize'] == 0]))\n",
    "print(len(df[df['fLength'] == 0]))\n",
    "print(len(df[df['fWidth'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['fWidth'] < 1].hist(column='fWidth', bins=100)\n",
    "df[df['fWidth'] == 0].hist(column='fSize', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(data=df.corr(),annot=True,cmap=\"bone\",linewidths=1,fmt=\".2f\",linecolor=\"gray\")\n",
    "plt.title(\"Correlation Map\",fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()      # lightest and darkest cells are most correlated ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df,\n",
    "             palette=\"husl\",\n",
    "             hue=\"fClass\",\n",
    "             vars=headers)\n",
    "plt.suptitle(\"PairPlot of Data Without Std. Dev. Fields\",fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(df[df.fWidth == 0].index, inplace=True) # Drops rows where fWidth == 0\n",
    "\n",
    "\"\"\"\n",
    "Y = df.fClass.values\n",
    "df.drop([\"fClass\"],axis=1,inplace=True) # Drops column fClass\n",
    "X = df.values\n",
    "\"\"\"\n",
    "\n",
    "Y = df[\"fClass\"].values\n",
    "X = df.drop([\"fClass\"],axis=1).values\n",
    "\n",
    "# Y = df_rescaled[\"rad\"].values\n",
    "# X = df_rescaled.drop([\"rad\"],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame rows, train_test_split already does that if we use random_state\n",
    "# df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18069, 10)\n",
      "(18069,)\n",
      "(951, 10)\n",
      "(951,)\n"
     ]
    }
   ],
   "source": [
    "# Scaling, we might use something different\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.05,random_state=42)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8002103049421662\n",
      "0.8407831891320519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(solver=\"sag\",C=0.1,penalty=\"l2\", tol = 1e-20, max_iter = 100, random_state = 100)\n",
    "lr_model.fit(x_train,y_train)\n",
    "lr_score = lr_model.score(x_test,y_test)\n",
    "lr_pred = lr_model.predict(x_test)\n",
    "print(lr_score)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_score = roc_auc_score(y_test,lr_model.predict_proba(x_test)[:, 1])\n",
    "print(roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_lr = confusion_matrix(y_test,lr_pred)\n",
    "\n",
    "plt.figure(figsize=(24,12))\n",
    "plt.suptitle(\"Confusion Matrixes\",fontsize=24)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "sns.heatmap(cm_lr,cbar=False,annot=True,cmap=\"CMRmap_r\",fmt=\"d\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 12)                132       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 78        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 217\n",
      "Trainable params: 217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of weights after calling the model: 6\n"
     ]
    }
   ],
   "source": [
    "# look for theory about no of parameters vs data rows\n",
    "# test different activation functions\n",
    "# check classifiers\n",
    "# check general pourpose layers instead of dense ones\n",
    "# check loss function\n",
    "# check optimizer for loss function\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation=tf.nn.relu, input_shape=(10,)))\n",
    "model.add(Dense(6, activation=tf.nn.sigmoid))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#non-working attempt at a custom loss function as in: https://towardsdatascience.com/explicit-auc-maximization-70beef6db14e\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     # calculate loss, using y_pred\n",
    "#     alpha = tf.sigmoid(y_pred @ tf.transpose(y_pred))\n",
    "#     beta = np.maximum(y_true @ np.ones(y_true.shape).T - np.ones(y_true.shape) @ y_true.T)\n",
    "#     loss = - tf.reduce_mean(alpha * beta, 0)\n",
    "#     return loss\n",
    "# model.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "print(\"Number of weights after calling the model:\", len(model.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1807/1807 [==============================] - 3s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 2/10\n",
      "1807/1807 [==============================] - 3s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 3/10\n",
      "1807/1807 [==============================] - 3s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 4/10\n",
      "1807/1807 [==============================] - 2s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 5/10\n",
      "1807/1807 [==============================] - 2s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 6/10\n",
      "1807/1807 [==============================] - 2s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 7/10\n",
      "1807/1807 [==============================] - 2s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 8/10\n",
      "1807/1807 [==============================] - 2s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 9/10\n",
      "1807/1807 [==============================] - 2s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n",
      "Epoch 10/10\n",
      "1807/1807 [==============================] - 2s 1ms/step - loss: 9.9956 - accuracy: 0.3520 - val_loss: 10.1049 - val_accuracy: 0.3449\n"
     ]
    }
   ],
   "source": [
    "dnn_history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dnn_history.history['accuracy'])\n",
    "plt.plot(dnn_history.history['val_accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Training data', 'Validation data'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 1ms/step - loss: 10.1049 - accuracy: 0.3449\n",
      "['loss', 'accuracy']\n",
      "dnn_score:  [10.104883193969727, 0.34490010142326355]\n",
      "roc score:  0.5115834083701992\n"
     ]
    }
   ],
   "source": [
    "dnn_score = model.evaluate(x_test, y_test)\n",
    "dnn_pred = model.predict(x_test)\n",
    "print(model.metrics_names)\n",
    "print(\"dnn_score: \",dnn_score)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_score = model.predict(x_test)\n",
    "roc_score = roc_auc_score(y_test,y_score)\n",
    "print(\"roc score: \", roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
